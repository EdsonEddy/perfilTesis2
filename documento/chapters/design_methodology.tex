\chapter{DISEÑO METODOLOGICO}

\section{Definicion de problemas}
Para el diseño del algoritmo se debe tomar en cuenta las transformaciones o métodos de ofuscacion de código de fuente que se presentaron en el capitulo anterior. Una transformación consiste en la modificación en las instrucciones del código con el fin de ocultar la similitud con otro. Estas transformaciones son tratados como problemas a resolverse.

\begin{itemize}
  \item[] \textbf{Problema 1.} Cambios en el formato del código. Es decir agregar o eliminar, saltos de linea, sangrías o espacios.
  \item[] \textbf{Problema 2.} Cambios en los comentarios del código. Es decir agregar, eliminar y modificar comentarios.
  \item[] \textbf{Problema 3.} Cambios en los nombres de los identificadores. Es decir cambiar los nombres de las varibales, constantes, nombres de funciones, nombres de clases, etc.
  \item[] \textbf{Problema 4.} Cambios en el orden de las declaraciones de las variables.
  \item[] \textbf{Problema 5.} Agregar instrucciones innecesarias. Es decir agregar declaraciones de variables que no se utilizan en el programa.
  \item[] \textbf{Problema 6.} Reemplazo de la llamada de un procedimiento por el procedimiento.
  \item[] \textbf{Problema 7.} Cambios en el orden de los operadores u operandos.
  \item[] \textbf{Problema 8.} Reemplazo en las estructuras de control por equivalentes.
\end{itemize}

El problema a resolver es: Dados dos archivos de codigo fuente A y B, donde estos archivos pueden tener o no transformaciones. Se quiere encontrar el porcentaje similitud entre los dos archivos dados.

\section{Especificación no formal del algoritmo}
El algoritmo SCED, tiene como entrada: Dos archivos de código fuente A y B, y un numero D en el rango de $[0 \twodots 100]$ que representa la precisión de la médicion. Como salida un numero en el rango de $[0 \twodots 100]$, que representa el porcentaje de similitud entre los dos archivos.

\input{figures/espAlg}

\section{Diseño del algoritmo}
El algoritmo SCED utiliza la distancia de Levenshtein como medida para cuantificar la similitud entre dos archivos de codigo fuente previamente tokenizados, al contar el minimo de operaciones requeridas para transformar un codigo fuente en otro. Las operaciones consisten en insertar, eliminar y reemplazar tokens en los codigos fuente. En la figura \ref{diseñoAlg} se muestra el diagrama de funcionamiento del algoritmo, en el cual se muestra las fases del algoritmo. Las fases consisten en: La tokenización de los códigos fuente, mediante un Lexer. El calculo de la distancia de edicion de las secuencias de tokens,  Levenshtein. Finalmente el calculo del porcentaje de similitud.

\input{figures/diseñoAlg}

Resumiendo el funcionamiento del algoritmo, el lexer tiene como funcion convertir el codigo fuente de los archivos en secuencias de tokens. Seguido con las secuencia de tokens del primer archivo y segundo archivo, se realizara el calculo de la distancia de edicion de las secuencias de tokens, obteniendo el valor numerico de similitud. Finalmente con el valor numerico se calcula el porcentaje de similitud de los codigos fuente. En la tabla \ref{algorithms_problems} se muestra un descripción general de la solucion a los problemas propuestos en la seccion anterior, mediante los algoritmos presentados en el capitulo anterior.

\input{tables/algorithms_problems}

\subsection{Tokenización y clasificacion}
En esta fase se eliminan los elementos innecesarios que tenga el código fuente como: Los comentarios, saltos de linea que no contengan instrucciones, espacios en blanco, etc. Esto se debe a que estas lineas no serán utilizadas para la comparación de similitud. Al mismo tiempo se realizara la tokenización y clasificacion de los tokens, los tokens generados son tuplas que contienen dos valores: El tipo de token, la cadena del token. A continuacion se muestra como se clasificaran los tokens:
\begin{itemize}
  \item \textbf{Token.text} para datos de tipo texto como: Espacios en blanco y  tabulaciones.
  \item \textbf{Token.keyword} para cualquier palabra clave o reservada.
  \item \textbf{Token.name.function} para nombres de funciones.
  \item \textbf{Token.name.variable} para nombres de variables.
  \item \textbf{Token.literal.string} para cualquier cadena.
  \item \textbf{Token.literal.number} para cualquier numero entero o flotante.
  \item \textbf{Token.operator} para cualquier operador.
  \item \textbf{Token.punctuation} para cualquier simbolo de puntuacion.
  \item \textbf{Token.comment} para cualquier comentario.
  \item \textbf{Token.other} para tokens no clasificados.
\end{itemize}

\input{figures/classToken}

El procedimiento $\proc{Lexer}$ tiene como entrada un archivo de código fuente \id{file}, y de salida un arreglo de tokens clasificados.

\input{pseudocode/Lexer}

\subsection{Cálculo de la distancia de edicion de secuencias de tokens}
En esta fase se calcula la distancia de edicion. A partir de la definición de la distancia de Levenshtein, podemos definir la distancia de edicion de secuencias de tokens como: Dadas dos secuencias de tokens $A[1 \twodots m]$ y $B[1 \twodots n]$ y operaciones de transformación y sus costos. La distancia de edición entre las secuencias $A$ y $B$ es el costo mínimo de operaciones que transforma $A$ en $B$. Las operaciones consisten eliminar, insertar y reemplazat tokens en las secuencias.

Para el calculo de la distancia de edicion de secuencias de tokens utilizaremos un algoritmo de programación dinamica, el algoritmo sera adaptado al calculo de la distancia de edicion de secuencias de tokens. El valor calculado por este modulo es la medida de similitud que se utilizara para determinar si dos códigos fuente son similares. Los valores del rango se muestran en la figura \ref{intervalLev}. Donde \id{minScore} representa que no necesitaron operaciones de transformación, lo que significa que las dos secuencias son exactamente iguales, y \id{maxScore} representa que se utilizaron el maximo numero de operaciones de transformación, lo que significa que las dos secuencias son muy diferentes.

\input{pseudocode/Get-Edit-Distance}

\subsection{Calculo del porcentaje de similitud}
A partir del valor obtenido en el calculo de la distancia de edicion, podemos calcular el porcentaje similitud entre dos códigos fuente. Dados dos archivos de código fuente \id{fileA} y \id{fileB}, la similitud entre los códigos esta dado por:
\begin{equation*}
a = \proc{num-tokens}(\id{file-A})
\end{equation*}
\begin{equation*}
b = \proc{num-tokens}(\id{file-B})
\end{equation*}
\begin{equation}
\id{Percentage} = (1 - \frac{\proc{get-edit-distance}(a, b)}{\proc{max}(a, b)}) * 100
\label{perSCED}
\end{equation}

En la ecuacion \ref{perSCED} se muestra el procentaje del indice de similitud, el cual determina la similitud entre dos archivos de codigo fuente.

\subsection{Algoritmo SCED}
Con los modulos $\proc{Lexer}$ y $\proc{Get-Edit-Distance}$ podemos calcular el porcentaje de similitud entre dos archivos. A continuación la implementación en pseudocódigo del algoritmo propuesto.

\input{pseudocode/SCED-Algorithm}

\section{Implementación en Python del algoritmo}
\subsection{Pygments}
\id{Pygments} es una libreria de codigo abierto escrita en el lenguaje de programacion de Python, \cite{brandl2022} explica que \id{Pygments} es un resaltador de sintaxis generico, es utilizado en foros, wikis u otros aplicaciones que necesitan embellecer el codigo fuente entre sus características se tiene:
\begin{itemize}
  \item Admite una amplia gama de lenguajes son 533 en total, y otros formatos de texto.
  \item Soporte nuevos lenguajes y formatos, se agrega facilmente, utilizan un mecanismo de lectura simple basado en expresiones regulares.
  \item Tiene varios formatos de salida disponibles, entre ellos secuencias \id{HTML}, \id{RTF}, \id{Latex} y \id{ANSI}.
  \item se puede utilizar como herramienta de linea de comandos y como biblioteca.
\end{itemize}
La biblioteca de \id{Pygments} tiene un modulo para el analisis lexico llamado \id{pygments.lexers}. El modulo se encarga de convertir un archivo de codigo fuente en un arreglo de tokens. Tambien tiene un modulo para el manejo de tokens llamado \id{pygments.token}.
\subsection{Modulo Lexer}
En este modulo se hace uso de la libreria \id{pygments} para la tokenizacion y clasificacion de los tokens del codigo fuente. Se implementa un procedimiento para calcular el valor hash de las cadenas de los tokens el procedimiento se llama $\proc{compute-hash}$.

\lstinputlisting[caption={Lexer},label={lst:lexer}, language=Python]{programs/design/lexer.py}
\subsection{Modulo Sequence-alignment}
En este modulo se implementan algoritmos de programacion dinamica que resuelven el problema de la distancia de edicion de dos secuencias. En la implementación  se toma en cuenta la reduccion de complejidad espacial y temporal, presentados en el capitulo anterior.
\lstinputlisting[caption={Sequence-alignment},label={lst:alignment}, language=Python]{programs/design/sequence_alignment.py}

La funcion $\proc{levenhtein-min-space-time}$ es la que se utilizara para el calulo de la distancia de edicion de secuencias, donde tiene como parametro de entrada dos secuencias de tokens y un numero \id{D} en el rango $[0, \twodots, 100]$ que representa la precisión en el calculo de la distancia de edicion.

\subsection{Modulo SCED-Algorithm}
En este modulo se aplican las anteriores implementaciones de algoritmos, y se calcula el porcentaje de similitud de dos codigos fuente.
\lstinputlisting[caption={SCED-Algorithm},label={lst:scam_algorithm}, language=Python]{programs/design/sced_algorithm.py}

\section{Análisis de complejidad}
A continuación el análisis de la complejidad temporal y espacial de cada uno de los procedimientos, la complejidad fue calculada en notación Big $O$.
\subsection{Complejidad temporal}
\begin{enumerate}
  \item El modulo $\proc{Lexer}$ tiene complejidad temporal de $O(t)$ donde t representa el numero de tokens que contiene el archivo.
  \item El modulo $\proc{Get-Edit-Distance}$ tiene complejidad temporal de $O(m*d)$ donde $m$ y $d \leq n$, representan el número de elementos de la primera secuencias de tokens y el numero de columnas adyacentes a la diagonal principal de $m*n$, y $n$ es el numero de elementos de la segunda secuencia.
  \item El modulo $\proc{SCED-Algorithm}$ hace uso de los anteriores metodos, entonces la complejidad temporal es de $O(t + (m*d))$.
\end{enumerate}
\subsection{Complejidad espacial}
\begin{enumerate}
  \item El modulo $\proc{Lexer}$ tiene complejidad espacial de $O(t)$ donde t representa el numero de tokens que contiene el archivo.
  \item El modulo $\proc{Get-Edit-Distance}$ tiene complejidad espacial de $O(\proc{min}(m,n))$ donde $m$ y $n$, representan el número de elementos de las secuencias de tokens.
  \item El modulo $\proc{SCED-Algorithm}$ hace uso de los anteriores metodos, entonces la complejidad espacial es de $O(t + \proc{min}(m,n))$.
\end{enumerate}
